---
layout: post
title: "Large language models for healthcare"
subtitle: "Healthcare Domain-Specific Language Models: Fine-tuning Methods"
date: 2024-01-27 00:00:00 +0900
categories: AI Healthcare
tags:
    - LLM
    - Healthcare
    - Medical AI
    - Domain-Specific Models
    - Fine-tuning
    - Medical NLP
description: "Fine-tuning Large Langauge Models (LLMs) for medical domain : training approaches, medical datasets"
toc: true
toc_sticky: true
---


# MED42-V2: A SUITE OF CLINICAL LLMS 요약정리

[모델 다운로드 링크](https://huggingface.co/m42-health)

## Stage 1: Clinical Fine-tuning Stage 

### 목적
- 의료 도메인에 대한 전문적 이해력 향상
- 의료 용어 및 개념에 대한 이해도 개선
- 의료 관련 질문 회피 성향 극복

### 사용 데이터셋
1. 의료 도메인 (73.50%)
   - 의학 교육: MedMCQA, Medical Flashcards
   - 연구 문헌: CORD-19, PubMedQA
   - 의료 지식: Medical-Instruction, StackExchange
   - 총 952,942 샘플

2. 일반 도메인 (26.50%)
   - SlimOrca 시리즈 (T0, Flan, CoT)
   - Ultrachat
   - 총 343,529 샘플

### Fine-tuning 방법
1. **학습 방식**
   - Full Parameter Fine-tuning (전체 파라미터 업데이트)
   - Auto-regressive Loss 사용
   - "Loss is backpropagated only on output tokens" 
     - 응답 생성만 학습하고 프롬프트 생성은 학습하지 않도록 설계
   - No Parameter-Efficient Methods (LoRA 등 미사용)

2. **데이터 처리**
   - 8192 토큰 길이로 데이터 청크화
   - Llama3/3.1의 기존 프롬프트 형식 유지
     - system, assistant, user 필드 포함

3. **학습 설정**
   - 2 epochs 진행
   - AdamW optimizer 사용
   - Linear warmup with Cosine decay 스케줄러

## Stage 2: Preference Alignment Stage

### 목적
- 모델 출력을 사용자 기대와 일치
- 윤리적 가이드라인 준수 향상
- 편향되지 않은 의료 정보 제공 능력 개선

### 사용 데이터셋
1. UltraFeedback 데이터셋
2. Snorkel-DPO 데이터셋
   - Stage 1: 기본 정렬
   - Stage 2: 중간 정렬
   - Stage 3: 최종 정렬

### Fine-tuning 방법
- Direct Preference Optimization (DPO) 사용
- 반복적 정렬 과정 (3단계)
- 각 단계마다 이전 모델을 보상 모델로 활용
- Huggingface Alignment Handbook 라이브러리 활용

## 하이퍼파라미터
1. 8B 모델
   - GPU: 2 x 8 H100s
   - Learning Rate: 5×10^-6
   - 배치 사이즈: 256

2. 70B 모델
   - GPU: 4-6 x 8 H100s
   - Learning Rate: 1×10^-6
   - 배치 사이즈: 128

## 결과
- 의료 분야 벤치마크에서 GPT-4 포함 기존 모델 대비 우수한 성능
- MMLU-Pro, MedQA, USMLE 등에서 높은 성능
- 실제 임상 환경 활용 가능성 확인
